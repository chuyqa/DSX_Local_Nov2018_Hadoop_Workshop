{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports needed for the cells which run locally on Watson Studio.\n",
    "\n",
    "import dsx_core_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Create_Livy_Session'></a>\n",
    "## Create Remote Livy Session\n",
    "\n",
    "First, let's get a list of registered Hadoop Integration systems. Look for a system that has the `imageId` of your custom image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Hadoop systems: \n",
      "\n",
      "        systemName LIVYSPARK  LIVYSPARK2                  imageId\n",
      "0   durotar-hdp301            livyspark2                         \n",
      "1  asgardia-hdp264            livyspark2  dsx-scripted-ml-python2\n"
     ]
    }
   ],
   "source": [
    "DSXHI_SYSTEMS = dsx_core_utils.get_dsxhi_info(showSummary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "myConfig={\n",
    " \"queue\": \"default\",\n",
    " \"driverMemory\": \"2G\",\n",
    " \"numExecutors\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkmagic has been configured to use https://asgardian-edge.fyre.ibm.com:8443/gateway/jalv-dsx121g-master-1/livy2/v1 with image Jupyter with Python 2.7, Scala 2.11, R 3.4.3\n",
      "success configuring sparkmagic livy.\n"
     ]
    }
   ],
   "source": [
    "# Set up sparkmagic to connect to the selected registered HI\n",
    "# system with the specified configs.\n",
    "dsx_core_utils.setup_livy_sparkmagic(\n",
    "  system=\"asgardia-hdp264\", \n",
    "  livy=\"livyspark2\",\n",
    "  imageId=\"dsx-scripted-ml-python2\",\n",
    "  addlConfig=myConfig)\n",
    "\n",
    "# (Re-)load spark magic to apply the new configs.\n",
    "%reload_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>450</td><td>application_1538175267044_0064</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://shad1.fyre.ibm.com:8088/proxy/application_1538175267044_0064/\">Link</a></td><td><a target=\"_blank\" href=\"http://shad2.fyre.ibm.com:8042/node/containerlogs/container_e25_1538175267044_0064_01_000001/user1\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "session_name = 'workshop-part1'\n",
    "livy_endpoint = 'https://asgardian-edge.fyre.ibm.com:8443/gateway/jalv-dsx121g-master-1/livy2/v1'\n",
    "%spark add -s $session_name -l python -k -u $livy_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create and use custom functions remotely\n",
    "\n",
    "**run_command** - Simple wrapper to subprocess, to run a linux command whithin the Driver YARN Container\n",
    "\n",
    "**spark_dfs_topandas** - Sample function that takes 2 Spark DFs and returns 2 Pandas DFs, necessary for ML tools such as Scikit which can only use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "def run_command(command, sleepAfter=None):        \n",
    "    p = Popen(command, shell=True, stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)\n",
    "    output = p.stdout.read()\n",
    "    print(output)\n",
    "    if (sleepAfter != None):\n",
    "        time.sleep(sleepAfter)\n",
    "        \n",
    "def spark_dfs_topandas(DF1,DF2):\n",
    "    return DF1.toPandas(),DF2.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For reference / debugging: Print out the name of the Hadoop node to which the remote session has been assigned. \n",
    "When \"local\" files are created within the remote session, they will be written to this node. All of the Yarn container artifacts (workspace and temp files) will exist on this node, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shad2.fyre.ibm.com\n",
      "\n",
      "/hadoop/yarn/local/usercache/user1/appcache/application_1538175267044_0064/container_e25_1538175267044_0064_01_000001"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "run_command(\"hostname -f\")\n",
    "run_command(\"pwd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2. Create quickens_demo_utils package and add via sc.addPyFile\n",
    "\n",
    "Once a set of functions are \"stable\" and ready to package, you can use a Python setup.py file to create a new \"quickens_demo_utils\" python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quickens_demo_utils-0.1.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../packages/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The \"quickens_demo_utils\" package can now be imported locally (DSX) to test it was added to the image properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from quickens_demo_utils import qutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the demo_utils tar to HDFS\n",
    "\n",
    "Note: This does not require admin privileges. \n",
    "\n",
    "As a user, upload the tar.gz to an accessible HDFS directory (Such as your /user/ directory). This allows end users to replace the file / test it before making it a part of a base DSX Runtime Image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://asgardian-edge.fyre.ibm.com:8443/gateway/jalv-dsx121g-master-1/webhdfs/v1', 'https://durotar-edge.fyre.ibm.com:8443/gateway/jalv-dsx121g-master-1/webhdfs/v1']\n"
     ]
    }
   ],
   "source": [
    "# Show registered WebHDFS Secure URLS which logged in user has access to:\n",
    "import dsx_core_utils\n",
    "dsx_core_utils.list_dsxhi_webhdfs_endpoints();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dsxlocal_file_location=\"../packages/python/quickens_demo_utils-0.1.tar.gz\"\n",
    "dsxhi_upload_hdfs_location=\"/user/user1/quickens_demo_utils-0.1.tar.gz\"\n",
    "webhdfs_endpoint=\"<your cluster webhdfs output from above>\"\n",
    "\n",
    "dsx_core_utils.hdfs_util.upload_file(webhdfs_endpoint, dsxlocal_file_location, dsxhi_upload_hdfs_location )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the zip from HDFS to a new Livy Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session_name = 'workshop-part1'\n",
    "livy_endpoint = HI_CONFIG['LIVY']\n",
    "webhdfs_endpoint = HI_CONFIG['WEBHDFS']\n",
    "%spark add -s $session_name -l python -k -u $livy_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "sc.addPyFile(\"/user/user1/quickens_demo_utils-0.1.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "from quickens_demo_utils import qutils"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with DSX Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
