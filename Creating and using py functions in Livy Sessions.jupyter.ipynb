{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='Create_Livy_Session'></a>\n",
    "## Create Remote Livy Session\n",
    "\n",
    "First, import `dsx_core_utils` and show a summary of all available DSXHI Endpoints to use within this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Hadoop systems: \n",
      "\n",
      "        systemName LIVYSPARK  LIVYSPARK2                  imageId\n",
      "0   durotar-hdp301            livyspark2  dsx-scripted-ml-python3\n",
      "1  asgardia-hdp264            livyspark2  dsx-scripted-ml-python2\n"
     ]
    }
   ],
   "source": [
    "import dsx_core_utils\n",
    "DSXHI_SYSTEMS = dsx_core_utils.get_dsxhi_info(showSummary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then, define any additional Spark Configurations. See [Livy Sessions REST API](https://livy.incubator.apache.org/docs/latest/rest-api.html) for additional properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkmagic has been configured to use https://asgardian-edge.fyre.ibm.com:8443/gateway/jalv-dsx121g-master-1/livy2/v1 \n",
      "success configuring sparkmagic livy.\n"
     ]
    }
   ],
   "source": [
    "myConfig={\n",
    " \"queue\": \"default\",\n",
    " \"driverMemory\": \"2G\",\n",
    " \"numExecutors\": 1\n",
    "}\n",
    "\n",
    "# Set up sparkmagic to connect to the selected registered HI\n",
    "# system with the specified configs.\n",
    "dsx_core_utils.setup_livy_sparkmagic(\n",
    "  system=\"asgardia-hdp264\", \n",
    "  livy=\"livyspark2\",\n",
    "  addlConfig=myConfig)\n",
    "\n",
    "# (Re-)load spark magic to apply the new configs.\n",
    "%reload_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>454</td><td>application_1538175267044_0068</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://shad1.fyre.ibm.com:8088/proxy/application_1538175267044_0068/\">Link</a></td><td><a target=\"_blank\" href=\"http://shad2.fyre.ibm.com:8042/node/containerlogs/container_e25_1538175267044_0068_01_000001/user1\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "session_name = 'workshop-part1'\n",
    "livy_endpoint = 'https://asgardian-edge.fyre.ibm.com:8443/gateway/jalv-dsx121g-master-1/livy2/v1'\n",
    "%spark add -s $session_name -l python -k -u $livy_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Create and use custom functions remotely**\n",
    "\n",
    "Lets create 2 simple functions:\n",
    "\n",
    "- **run_command** - Simple wrapper to subprocess, to run a linux command whithin the Driver YARN Container\n",
    "\n",
    "- **spark_dfs_topandas** - Sample function that takes 2 Spark DFs and returns 2 Pandas DFs. ToPandas() is generally not advisable, as it will be resource intensive on the Spark Driver. Some ML Tools however, require the DataFrame to be a Pandas DF, so this is merelly an example for such scenarios. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark -s $session_name\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "def run_command(command, sleepAfter=None):        \n",
    "    p = Popen(command, shell=True, stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)\n",
    "    output = p.stdout.read()\n",
    "    print(output)\n",
    "    if (sleepAfter != None):\n",
    "        time.sleep(sleepAfter)\n",
    "        \n",
    "def spark_dfs_topandas(DF1,DF2):\n",
    "    return DF1.toPandas(),DF2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shad2.fyre.ibm.com\n",
      "\n",
      "/hadoop/yarn/local/usercache/user1/appcache/application_1538175267044_0068/container_e25_1538175267044_0068_01_000001"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "\n",
    "run_command(\"hostname -f\")\n",
    "run_command(\"pwd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Transfer the `quicken_demo_utils.py` script which was saved above, to your HDFS user directory.\n",
    "\n",
    "- Use cell magic  `!ls ../scripts`   to see the relative path of the script which was just saved:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quicken_demo_utils.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- In a new cell, Show registered WebHDFS Secure URLS which logged in user has access to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://asgardian-edge.fyre.ibm.com:8443/gateway/jalv-dsx121g-master-1/webhdfs/v1', 'https://durotar-edge.fyre.ibm.com:8443/gateway/jalv-dsx121g-master-1/webhdfs/v1']\n"
     ]
    }
   ],
   "source": [
    "import dsx_core_utils\n",
    "dsx_core_utils.list_dsxhi_webhdfs_endpoints();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- Use `dsx_core_utils.hdfs_util.upload_file` to upload a file from DSX to your HDFS desired path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "upload success\n"
     ]
    }
   ],
   "source": [
    "dsxlocal_file_location=\"../scripts/quicken_demo_utils.py\"\n",
    "dsxhi_upload_hdfs_location=\"/user/user1/quicken_demo_utils.py\"\n",
    "webhdfs_endpoint=\"https://asgardian-edge.fyre.ibm.com:8443/gateway/jalv-dsx121g-master-1/webhdfs/v1\"\n",
    "\n",
    "dsx_core_utils.hdfs_util.upload_file(webhdfs_endpoint, dsxlocal_file_location, dsxhi_upload_hdfs_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4. Test the .py file via sc.addPyFile in a new Livy Session\n",
    "\n",
    "- Delete the old session with `%spark cleanup`\n",
    "- Create a new session with `%spark add`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '500' from https://asgardian-edge.fyre.ibm.com:8443/gateway/jalv-dsx121g-master-1/livy2/v1/sessions/456 with error payload: \n"
     ]
    }
   ],
   "source": [
    "%spark cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>456</td><td>application_1538175267044_0070</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://shad1.fyre.ibm.com:8088/proxy/application_1538175267044_0070/\">Link</a></td><td><a target=\"_blank\" href=\"http://shad1.fyre.ibm.com:8042/node/containerlogs/container_e25_1538175267044_0070_01_000001/user1\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%spark add -s $session_name -l python -k -u $livy_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- Add the \"quicken-demo-utils.py\" file to the spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "sc.addPyFile(\"hdfs:///user/user1/quicken_demo_utils.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "import quicken_demo_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- Test the imported utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shad1.fyre.ibm.com\n",
      "\n",
      "/hadoop/yarn/local/usercache/user1/appcache/application_1538175267044_0070/container_e25_1538175267044_0070_01_000001"
     ]
    }
   ],
   "source": [
    "%%spark -s $session_name\n",
    "\n",
    "utils.run_command(\"hostname -f\")\n",
    "utils.run_command(\"pwd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with DSX Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
